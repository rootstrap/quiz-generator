
Relevant concepts to keep in mind about Data Science 
What is data science? 

ANALYTICS
DATA SCIENCE
BUSINESS
MATH & PROGRAMMING

Needed skills in data science:
1- Business understanding to well-define the problem
2- Statistics knowledge to understand how algorithms work
3- Analytics expertise to interpret the results 

Data by itself is meaningless if we cannot extract the value from it. 
Different between concepts 

Artificial intelligence: is a more general concept that consists of the use of machines to perform tasks based on algorithms. 

Machine learning: is a part of AI that focuses on the ability of machines to learn from past data and correct themselves. 

Deep Learning: is a subset of machine learning, where multi-layer neural networks are used to apply pattern matching. 

NLP - Natural Language processing: Is used to extract valuable information from text 
Used in sentiment analysis, recommendation systems, chatbots, speech recognition, text summarization. 

Speech Recognition: recognition and translation of spoken language into text by computers

Computer vision: A field of AI that trains machines to identify and classify objects in order to understand the visual world through digital images and they are able to react in function of what they interpret. 






What is big data? 
Big Data is a combination of these 5 characteristics, called the “5Vs” of Big Data. 
Volume: huge amount of information
Variety: data comes from different sources: audio files, photography, complex documents, tweets, comments, network relationships, geocodes, purchases, etc. 
Velocity: data is being created quickly and it needs to be processed quickly
Veracity: data might have inconsistencies
Value: datasets might need to be combined and processed to generate value 

Data scale 
Data creation: how fast the data is being generated . The world's data is doubling every two years. 
Data storage: how much data we keep. 
Data processing: how much data we use 
Data consumption: demand of data 

How large ? Depends on the capabilities of the organization to create/store.process and consumption . For some organizations hundreds of gigabytes might be a huge amount of data to handle, while for others it has to reach to tens or hundreds of terabytes before becoming difficult to manage.  

Change of paradigm: When working with big data there are some changes in the way that we store and process the data:
Duplication: The data might be duplicated in several formats.
Incomplete: The data might have missing information.
Inconsistent: The data might not be consistent.

But not always need big data. You only need the right data according to the problem. 
 You Don’t Need Big Data — You Need the Right Data
What is ETL?
Extract Transform Load - extract is the process of pulling data from the different sources, transform is the process of applying different transformations over the data and load is the process of storing the transformed information. 
This term might be found in ELT, which differs in the order of the performed activities. It does not mean that one is better than the other, it just depends on the problem, what is more convenient.  

Data Capture
Ways of capture data capture: 
Passive: the user does not realize that the data is being captured. For example, location data. 
Active:  the user is conscious that the data is being captured. 


Why data science?
Why is it important to apply data science? [1] 
Cost reduction
Time reduction
New product / Innovation
Support decisions
Real time value
Precision
Differentiate from competitors - keep up the market demand  

How to get value from data? 
There are different data strategies to transform your data into potential value. Which might be reflected in money is called data monetization. 
The different strategies are [2]: 
Keep the data privatery: Leverage data for internal operations 
Selling data as a product: 
sell data product to asset owners or their competitors 
Sell data product as “premium access”
Trading data (data exchange): trade data with partners for mutual benefits 
Making data open and free: make data available to users, this might generate value in another way

Data science steps
Business problem: understand the business context, understand the meaning of the data, talk with experts. Define the problem. 
Data acquisition: get the correct data to resolve the problem. 
Data preparation: prepare the data for the model 
Data fusion: put together data from different sources and formats
Data cleaning: apply transformations over the data, resolve missing information.
Exploratory analysis: make an exploratory analysis that helps you understand how the data is distributed, and detect outliers (out of common cases).  
Data modeling: this process consists of the application of different models that learn from the data (training phase) and then we test to see how well the model has learned. In function of those results, we improve the model (by tuning parameters) and perform the process again.  
Data visualization and Communication: once we reach a solution, we would need to show the client the information in a way that is clear to understand and communicate well the results. 
Deploy: once that we finished, we will put in production the model and we would need how the model will be maintained and monitored. Define processes to be followed when new data is available, define the lifecycle of the model. 
 
Research 
In order to understand the business and how to attack the problem, a first research will be needed. During this research prototypes might be performed in order to provide insights of which route to follow. 
Iterative process
The idea is to start with a simple model and do multiple iterations to improve the results. 

Data privacy 

Passive data capture brings data privacy concerns. It is questioned if is it legal to capture the data, and what are the legal uses that you can apply with it. Can you share the data with other parties? With which level of detail? How can the data be stored? 

Data privacy is different from Data Security. Data privacy refers to the fact of governing how data will be collected, used , shared and stored. Data security refers to the protection of data from external attackers.

This area is very vague and more decisions will have to be taken in order to protect people’s data. It is very delicate and you have to be careful.

Regulations
GDPR: European Data Protection Regulation applied since May 25th, 2018  Establishes data protection manuals for data from EU’s citizens. Determines that the data has to be secured, up to date, there should be transparency about the use of the data, and you should use the minimum amount of data for the job. So, you should receive the user's consent to use the data. In addition, people have the right to ask for deleting their data.

Review into bias in algorithmic decision-making : The Centre for Data Ethics and Innovation (CDEI) is an independent expert committee, set up and tasked by the UK government to investigate and advise on how we maximize the benefits of these technologies. They investigated the potential bias in decisions made by algorithms. This link (2020) includes a set of formal recommendations to the government.


Protected health information (PHI) under the US law refers to  any information about health that is created or collected and can be linked to a specific individual. The HIPAA Privacy Rule, explains how and when your PHI can or cannot use that data.


California Consumer Privacy Act (CCPA) since  June 1, 2020 any California consumer has the right to demand to see all the information that a company has saved about them. Also, a list of all the third parties that data has been shared with.


Gramm-Leach-Bliley Act (GLBA): Financial Modernization Act of 1999. It is a United States federal law that establishes that financial institutions have to explain how they share and protect their customers’ private information.


Brazilian General Data Protection Law (LGPD) is largely aligned to the EU General Data Protection Act (GDPR).
Argentina the Personal Data Protection Law (PDPL) includes the basic personal data rules. It follows international standards, and has been considered as granting adequate protection by the European Commission. Decree 1558 of 2001 includes regulations issued under the PDPL. Further regulations have been issued by the relevant agencies.

Uruguay: Ley 18.331 - Protección de Datos Personales y acción de Habeas Data


Types of machine learning
Types of ML
Different types of machine learning, based on the way that algorithm learns. Each type would be described in this section, providing specific cases.

Supervised learning
In supervised learning the main purpose is to predict or classify future data based on past data. Prediction: Examples of prediction are sales or prices, it always corresponds to a numerical value. 
Classification: it can be a situation of only two classes for example, for example yes or no questions, or more classes like determining the level of the water (Low, Medium, High). 

Unsupervised learning
In the case of unsupervised learning the model learns from data that does not include desired outputs. The model detects anomalies (anomaly detection), finds trends or patterns in the data, finds groups among data points creating clusters. 

Semi-supervised learning
It is intermediate between supervised and unsupervised, where the learning process starts with the tagged training data and after uses untagged data. It mixes both models.

Reinforcement learning
Labelled input/output pairs are not needed. The algorithm focuses on maximizing reward in a particular situation. The reinforcement agent decides what action to perform based on the given task. It learns from experience, correcting itself. This type of learning is used for example in autonomous vehicles. 

Models
Different machine learning models that are used in several types of machine learning. Here the most common are described. 

Regression: The basis for ML problems is an equation that tries to define the value of a variable in function of a linear combination of other variables.
Trees: It is based on a set of rules (if-then-else), that represents the decisions made by the algorithm which is represented by a tree structure.
Random Forest: a combination of trees.
KNN: K-nearest neighbor,groups into K neighbors which are nearby and assign a class to each group. 
Kmeans: tries to find the split of n data points into k groups
Hierarchical clustering: establishes a hierarchy between data points based on the distance
Neural networks: A neural network consists of many nodes connected between each other, which pass information between them, applying functions over this data. 




